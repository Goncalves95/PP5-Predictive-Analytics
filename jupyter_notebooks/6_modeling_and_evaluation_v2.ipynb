{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Modeling Sale Price using Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of CRISP-DM Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Fit and evaluate a regression model to predict the sale price based on the features in the dataset.\n",
    "* Perform hyperparameter tuning to optimize the model's performance.\n",
    "* Identify and analyze the most important features used in model training.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/house_prices_records.csv: The raw dataset containing house prices records.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* X_train and y_train: The training set features and target, respectively.\n",
    "* X_test and y_test: The testing set features and target, respectively.\n",
    "* Pipeline_regression: The trained machine learning pipeline to predict sale prices.\n",
    "* Feature_importance_plot: A plot visualizing the importance of each feature in the trained model.\n",
    "* Model_performance_evaluation.png: A plot evaluating the performance of the trained model on both the training and testing sets.\n",
    "* Regression_pipeline.pkl: The saved trained regression pipeline model for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load housing_records.csv into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"outputs/dataset/housing_records.csv\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML Pipeline: Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pipeline from scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import data cleaning modules from feature_engine\n",
    "# Data Cleaning\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "from feature_engine.outliers   import Winsorizer\n",
    "\n",
    "# Import feature engineering modules from feature_engine\n",
    "# Feature Engineering\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection, DropFeatures\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "# Import feature scaling module from scikit-learn\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import feature selection module from scikit-learn\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Import machine learning algorithms from scikit-learn and xgboost\n",
    "# ML algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def pipeline_optimization(model):\n",
    "    # Define a pipeline with multiple steps\n",
    "    pipeline_base = Pipeline([\n",
    "        \n",
    "        # Drop features 'EnclosedPorch' and 'WoodDeckSF'\n",
    "        (\"DropFeatures\", DropFeatures(features_to_drop=['EnclosedPorch', 'WoodDeckSF'])),\n",
    "\n",
    "        # Impute missing values with 0 for features '2ndFlrSF' and 'MasVnrArea'\n",
    "        (\"ArbitraryNumberImputer\",ArbitraryNumberImputer(arbitrary_number=0, variables=['2ndFlrSF','MasVnrArea'])),\n",
    "\n",
    "        # Impute missing categorical values with 'Unf' for features 'GarageFinish' and 'BsmtFinType1'\n",
    "        (\"CategoricalImputer\", CategoricalImputer(imputation_method='missing',fill_value='Unf', variables=['GarageFinish','BsmtFinType1'])),\n",
    "        \n",
    "        # Impute missing values with median for features 'BedroomAbvGr', 'GarageYrBlt', and 'LotFrontage'\n",
    "        (\"MedianImputation\", MeanMedianImputer(imputation_method='median', variables=['BedroomAbvGr' , 'GarageYrBlt', 'LotFrontage'])),\n",
    "\n",
    "        # Ordinal encode categorical features 'BsmtExposure', 'BsmtFinType1', 'GarageFinish', and 'KitchenQual'\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary', \n",
    "                                                     variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'])),\n",
    "\n",
    "        # Apply log transformation to features '1stFlrSF' and 'GrLivArea'\n",
    "        (\"LogTransformer\", vt.LogTransformer(variables=['1stFlrSF','GrLivArea'])),\n",
    "\n",
    "        # Apply Yeo-Johnson transformation to features 'BsmtUnfSF', 'GarageArea', and 'TotalBsmtSF'\n",
    "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(variables=['BsmtUnfSF','GarageArea','TotalBsmtSF'])),\n",
    "\n",
    "        # Apply power transformation to feature 'LotArea'\n",
    "        (\"PowerTransformer\", vt.PowerTransformer(variables=['LotArea'])),\n",
    "\n",
    "        # Winsorize feature 'GrLivArea' to reduce outliers\n",
    "        (\"Winsorizer\",Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=['GrLivArea'])),\n",
    "                                      \n",
    "        # Select features based on correlation and variance\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "\n",
    "        # Scale features using StandardScaler\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # Select features based on importance weights from the model\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "\n",
    "        # Train the machine learning model\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV from scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a class for hyperparameter optimization search\n",
    "class hyperparameter_optimization_search:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        # Initialize the class with models and parameters\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        # Iterate over each model and perform GridSearchCV\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            # Create a pipeline for the current model\n",
    "            model = pipeline_optimization(self.models[key])\n",
    "\n",
    "            # Get the parameters for the current model\n",
    "            params = self.params[key]\n",
    "            # Perform GridSearchCV with the current model and parameters\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            # Store the GridSearchCV object\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        # Define a helper function to create a row for the summary dataframe\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        # Initialize an empty list to store the rows\n",
    "        rows = []\n",
    "        # Iterate over each GridSearchCV object\n",
    "        for k in self.grid_searches:\n",
    "            # Get the parameters and scores from the GridSearchCV object\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            # Create a row for each parameter setting\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        # Create a dataframe from the rows\n",
    "        data = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        # Define the columns for the summary dataframe\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in data.columns if c not in columns]\n",
    "\n",
    "        # Return the summary dataframe and the GridSearchCV objects\n",
    "        return data[columns], self.grid_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import train_test_split from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Features (drop 'SalePrice' column)\n",
    "    data.drop(['SalePrice'], axis=1),\n",
    "    # Target variable ('SalePrice' column)\n",
    "    data['SalePrice'],\n",
    "    # Test set size (20% of the data)\n",
    "    test_size=0.2,\n",
    "    # Random seed for reproducibility\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
    "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search CV - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of machine learning models to search\n",
    "models_search = {\n",
    "    # Linear Regression model\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    # Decision Tree Regressor model with random state 0 for reproducibility\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    # Random Forest Regressor model with random state 0 for reproducibility\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    # Extra Trees Regressor model with random state 0 for reproducibility\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    # AdaBoost Regressor model with random state 0 for reproducibility\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
    "    # Gradient Boosting Regressor model with random state 0 for reproducibility\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    # XGB Regressor model with random state 0 for reproducibility\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "# Define a dictionary of hyperparameter search spaces for each model\n",
    "params_search = {\n",
    "    # No hyperparameters to search for Linear Regression\n",
    "    'LinearRegression': {},\n",
    "    # No hyperparameters to search for Decision Tree Regressor\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    # No hyperparameters to search for Random Forest Regressor\n",
    "    \"RandomForestRegressor\": {},\n",
    "    # No hyperparameters to search for Extra Trees Regressor\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    # No hyperparameters to search for AdaBoost Regressor\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    # No hyperparameters to search for Gradient Boosting Regressor\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    # No hyperparameters to search for XGB Regressor\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter optimization search\n",
    "search = hyperparameter_optimization_search(\n",
    "    # Dictionary of machine learning models to search\n",
    "    models=models_search, \n",
    "    # Dictionary of hyperparameter search spaces for each model\n",
    "    params=params_search\n",
    ")\n",
    "\n",
    "# Fit the hyperparameter optimization search to the training data\n",
    "search.fit(\n",
    "    # Features of the training set\n",
    "    X_train, \n",
    "    # Target variable of the training set\n",
    "    y_train, \n",
    "    # Evaluation metric for hyperparameter optimization (R-squared)\n",
    "    scoring='r2', \n",
    "    # Number of jobs to run in parallel (-1 means all available cores)\n",
    "    n_jobs=-1, \n",
    "    # Number of folds for cross-validation\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary of the hyperparameter optimization search\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(\n",
    "    # Sort the results by the mean score of each model\n",
    "    sort_by='mean_score'\n",
    ")\n",
    "\n",
    "# Print the summary of the hyperparameter optimization search\n",
    "print(grid_search_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best-performing model from the search summary\n",
    "best_model = grid_search_summary.iloc[0, 0]\n",
    "\n",
    "# Print the best-performing model\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store the models to search\n",
    "models_search = {\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "# Define a dictionary to store the hyperparameter search space for each model\n",
    "params_search = {\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        # Hyperparameter search space for GradientBoostingRegressor\n",
    "        'model__learning_rate': [0.05],  # Search for the best learning rate\n",
    "        # Other hyperparameters are commented out, but can be uncommented to explore more options\n",
    "        # 'odel__n_estimators': [50, 75, 100],  # Search for the best number of estimators\n",
    "        # 'odel__max_depth': [3, 5, 10],  # Search for the best maximum depth\n",
    "        # 'odel__min_samples_split': [2, 25, 50, 75],  # Search for the best minimum samples to split\n",
    "        # 'odel__min_samples_leaf': [1, 25, 50, 75],  # Search for the best minimum samples in a leaf\n",
    "        # 'odel__max_leaf_nodes': [None, 25, 50, 75],  # Search for the best maximum leaf nodes\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# documentation to help on hyperparameter list: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "# In a workplace project, you may consider more hyperparameters and spend more time in this step\n",
    "# https://inria.github.io/scikit-learn-mooc/python_scripts/ensemble_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter optimization search\n",
    "search = hyperparameter_optimization_search(models=models_search, params=params_search)\n",
    "\n",
    "# Fit the search object to the training data\n",
    "search.fit(X_train, y_train, \n",
    "           scoring='r2',  # Use R-squared as the evaluation metric\n",
    "           n_jobs=-1,  # Use all available CPU cores for parallel processing\n",
    "           cv=5  # Perform 5-fold cross-validation\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0, 0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_regressor_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thoroughly searching various combinations of these 6 parameters, we discovered that the 'learning_rate' parameter is the only one where the default value does not provide the best result when combined with the others. Instead, it works best at 0.05 rather than the default 0.1. All other parameters achieve optimal results using their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is structured to progressively filter and identify the features that offer the highest predictive power. Initially, we decided to exclude two features due to high levels of missing data: 'EnclosedPorch' and 'WoodDeckSF'. Then, the SmartCorrelationSelection step identifies additional features to drop: '1stFlrSF', 'GarageYrBlt', 'GrLivArea', and 'YearRemodAdd'. Lastly, the SelectFromModel step further refines the selection to the most crucial features for predicting the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # import matplotlib for plotting\n",
    "import seaborn as sns  # import seaborn for visualization\n",
    "sns.set_style('whitegrid')  # set seaborn style to whitegrid\n",
    "\n",
    "# get the number of data cleaning and feature engineering steps in the pipeline\n",
    "data_cleaning_feat_eng_steps = 10 \n",
    "\n",
    "# get the column names after data cleaning and feature engineering\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "# get the best features selected by the feature selection step\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()\n",
    "].to_list()\n",
    "\n",
    "# create a DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()],\n",
    "    'Importance': best_regressor_pipeline['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "# print the most important features and plot their importance\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "# plot the feature importance as a bar chart\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# function to evaluate regression performance on both train and test sets\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    # evaluate performance on train set\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    # evaluate performance on test set\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "\n",
    "# function to evaluate regression performance on a single dataset\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    # make predictions using the pipeline\n",
    "    prediction = pipeline.predict(X)\n",
    "    # print performance metrics\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
    "    print('Root Mean Squared Error:', np.sqrt(\n",
    "        mean_squared_error(y, prediction)).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# function to create scatter plots of actual vs predicted values for train and test sets\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    # make predictions on train and test sets\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    # create a figure with two subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    \n",
    "    # create scatter plot for train set\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "    sns.lineplot(x=y_train, y=y_train, color='red', ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Actual\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set\")\n",
    "\n",
    "    # create scatter plot for test set\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "    sns.lineplot(x=y_test, y=y_test, color='red', ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Actual\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set\")\n",
    "\n",
    "    # show the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_regressor_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics R2 are excellent for both the training set (0.886) and the test set (0.84), indicating no overfitting. The model meets the business criteria of an R2 score of 0.8 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised ML Pipeline Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've streamlined the ML pipeline to focus solely on the most impactful features identified by our analysis: 'OverallQual', 'TotalBsmtSF', '2ndFlrSF', and 'GarageArea'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineOptimization(model):\n",
    "    optimized_pipeline = Pipeline([\n",
    "\n",
    "        # Impute missing values for '2ndFlrSF' with 0\n",
    "        (\"ArbitraryNumberImputer\", ArbitraryNumberImputer(arbitrary_number=0, variables=['2ndFlrSF'])),\n",
    "\n",
    "        # Apply Yeo-Johnson transformation to 'GarageArea' and 'TotalBsmtSF'\n",
    "        (\"YeoJohnsonTransformer\", YeoJohnsonTransformer(variables=['GarageArea', 'TotalBsmtSF'])),\n",
    "\n",
    "        # Standardize the features\n",
    "        (\"scaler\", StandardScaler()),\n",
    "\n",
    "        # Add the machine learning model\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best parameters were determined in the previous notebook\n",
    "\n",
    "models_search = {\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        'model__n_estimators': [100],\n",
    "        'model__max_depth': [3],\n",
    "        'model__learning_rate': [0.05],\n",
    "        'model__min_samples_split': [2],\n",
    "        'model__min_samples_leaf': [1],\n",
    "        'model__max_leaf_nodes': [None],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the train and test datasets to include only the best features\n",
    "X_train = X_train.filter(best_features)\n",
    "X_test = X_test.filter(best_features)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n",
    "# Display the first 3 rows of the filtered training set\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train, scoring = 'r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_regression = grid_search_pipelines[best_model].best_estimator_\n",
    "pipeline_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, pipeline_regression)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving now the new pipeline like V2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = 'v2'\n",
    "file_path = f'outputs/ml_pipeline/predict_price/{version}'\n",
    "\n",
    "try:\n",
    "  os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained regression pipeline model to a file named regression_pipeline.pkl in the specified file_path using Joblib for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_regression, filename=f\"{file_path}/regression_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    \"\"\"\n",
    "    Function to generate scatter plots for regression model evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (array-like): Training features.\n",
    "    y_train (array-like): Training target.\n",
    "    X_test (array-like): Testing features.\n",
    "    y_test (array-like): Testing target.\n",
    "    pipeline (object): Trained regression pipeline model.\n",
    "    alpha_scatter (float, optional): Transparency of scatter plot points. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "    # Predict on training and testing data\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "    # Plot training data\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "    sns.lineplot(x=y_train, y=y_train, color='red', ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Actual\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set\")\n",
    "\n",
    "    # Plot testing data\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "    sns.lineplot(x=y_test, y=y_test, color='red', ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Actual\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set\")\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(f'{file_path}/model_performance_evaluation.png', bbox_inches='tight')\n",
    "\n",
    "# Call the function with the trained pipeline\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push files to Repo\n",
    "\n",
    "* git add .\n",
    "* git commit -m \" \" \n",
    "* git push "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
